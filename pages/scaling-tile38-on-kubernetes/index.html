<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta name="description" content="Scaling Tile38 with replication on kubernetes"><meta property="og:title" content="Scaling Tile38 on Kubernetes | Magpie dev"><meta property="og:site_name" content="Magpie dev"><meta property="og:type" content="website"><meta property="og:description" content="Scaling Tile38 with replication on kubernetes"><meta property="og:url" content="https://magpiedev.com"><meta property="og:image" content="https://magpiedev.com/pages/scaling-tile38-on-kubernetes/1.png?0.1284736927378749?0.40416483882500054"><link rel="stylesheet" href="/index.css?0.40416483882500054"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400"><link rel="shortcut icon" href="/images/logo.ico"><title>Scaling Tile38 on Kubernetes | Magpie dev</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-101774767-2', 'auto');
ga('send', 'pageview');</script></head><body><nav class="navbar"><div class="content"><a class="logo" href="/"><img class="image" src="/images/logo.svg"></a><a class="title" href="/">Magpie dev</a><div class="items"><a href="/posts">articles</a></div><!-- .items.right--><!--   a(href='/about' class=page_active === 'about' ? 'active' : '')--><!--     img(src='/images/github.svg')--></div></nav><div class="view"><div class="featured-box-component content-page"><div class="post-title">Scaling Tile38 on Kubernetes</div><div class="post-info"><div class="post-date">Aug 6th, 2018</div><div class="post-views"><div>Views:</div><iframe class="stats" src="https://stats.lacy.ninja/scaling-tile38-on-kubernetes/stats?output=text" scrolling="no"></iframe></div></div><div class="post-image" style="background-image: url(&quot;/pages/scaling-tile38-on-kubernetes/1.png?0.1284736927378749&quot;)"></div><h1 id="scaling-tile38-on-kubernetes">Scaling Tile38 on Kubernetes</h1>
<br />

<p><a href="https://github.com/tidwall/tile38">Tile38</a> is a geospatial database written in golang. It is designed to compute gis data with high throughput.</p>
<p>For our system we needed to autoscale Tile38 from 1 instance on kubernetes to 50 or more depending on load and requests.</p>
<p>Since Tile38 uses mostly CPU for calculations, it is easy to add a CPU based horizontal autoscaler on kubernetes to change the number of replicas available.</p>
<h2 id="leaderfollower-replication">leader/follower replication</h2>
<p>Tile38 supports basic leader/follower replication. The follower instances run the tile38 command:</p>
<p><code>FOLLOW leaderhost 9851</code></p>
<p>Where <code>leaderhost</code> is the IP or dns hostname of the leader, it will then autosync with the leader&#39;s AOF.</p>
<p>For the setup, there will be one leader read/write kubernetes Deployment with a service dedicated to it, and one follower/readonly Deployment with a service for it.</p>
<p>This diagram shows a general overview:</p>
<p><img src="./2.png" alt="diagram one -- kubernets leader/follower replication"></p>
<br />
`tile38.yaml`

<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 master service</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-write</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">9851</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">tile38-write</span>

<span class="hljs-meta">---</span>
</code></pre></div><p>That service will enable your API deployment to connect to <code>http://tile38-write:9851</code> or via tile38&#39;s redis adapter, <code>tile38-write:9851</code>.</p>
<p>The tile38 leader now needs a deployment of a single read/write Pod. This ensures that you are never writing to two instances that do not sync.</p>
<br />
`tile38.yaml`

<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 master deployment</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-write</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">tile38-write</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">"tile38/tile38:alpine"</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-write</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9851</span>
              <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-meta">---</span></code></pre></div><p>Once complete you can run <code>kubectl apply -f tile38.yaml</code> to setup the deployment and service. Verify that you can connect to it inside your cluster.</p>
<h4 id="now-for-the-autoscaling-read-follow-instances">Now for the autoscaling read (follow) instances.</h4>
<p>This service and deployment is what will take the brunt of all read requests, at times each pod may be using 100% of it&#39;s assigned CPU. For this setup I assigned one full CPU core to each Pod.</p>
<br />
`tile38.yaml`

<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 read service</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-read</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">9851</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">tile38-read</span>

<span class="hljs-meta">---</span></code></pre></div><p>Your instances can now connect to <code>http://tile38-read:9851</code> and all the requests will be balanced to all the Pods in the following Deployment:</p>
<br />
`tile38.yaml`

<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 read</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-read</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span> <span class="hljs-comment"># Initial replica count</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">tile38-read</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">"stevelacy/tile38:alpine"</span> <span class="hljs-comment"># Custom image that contains the check.py</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-read</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9851</span>
              <span class="hljs-attr">name:</span> <span class="hljs-string">tile38-read</span>
          <span class="hljs-attr">resources:</span>
            <span class="hljs-attr">limits:</span>
              <span class="hljs-attr">cpu:</span> <span class="hljs-number">1</span>
            <span class="hljs-attr">requests:</span>
              <span class="hljs-attr">cpu:</span> <span class="hljs-number">1</span>
          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span>
          <span class="hljs-attr">lifecycle:</span>
            <span class="hljs-attr">postStart:</span>
              <span class="hljs-attr">exec:</span>
                <span class="hljs-attr">command:</span> <span class="hljs-string">["python",</span> <span class="hljs-string">"/app/check.py"</span><span class="hljs-string">]</span> <span class="hljs-comment"># This is a custom script to ensure the replica "follows" the leader host</span>
          <span class="hljs-attr">readinessProbe:</span> <span class="hljs-comment"># This ensures that this Pod does not show it's state as "ready" until it follows and fully connects to the leader host</span>
            <span class="hljs-attr">exec:</span>
              <span class="hljs-attr">command:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">python</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">/app/check.py</span>
            <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
            <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">30</span>
            <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">5</span></code></pre></div><p>This deployment is using a custom image, <a href="https://hub.docker.com/r/stevelacy/tile38/">stevelacy/tile38:alpine</a></p>
<p>That image is using a python script for confirming that the leader is online: <a href="https://github.com/stevelacy/tile38-kubernetes-readiness">check.py</a></p>
<br />
> Note: it checks to see that the leader tile38 instance has 10 or more boundaries in it's system. If you wish to have fewer than 10 boundaries please modify as needed.


<h4 id="autoscaling">Autoscaling</h4>
<p>The core kubernetes autoscaler does a decent job of monitoring the cpu levels of the Pods and changing the requested replica counts as needed.</p>
<p>This autoscaling config will ensure that the replicas will start at 1 and scale up to a max of 10 when the average cpu crosses over 80%.</p>
<p>When the cpu load drops below 80% it will slowly start removing the excess replicas until it achieves constant 80% load or just one remaining replica.</p>
<br />
`tile38.yaml`
```
---

<h1 id="autoscale-pods-based-on-cpu-usage">Autoscale pods based on cpu usage</h1>
<p>apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: tile38-autoscaler
  namespace: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1beta1
    kind: Deployment
    name: tile38-read
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 80</p>
<div class="markdown-code"><pre><code class="lang-">

<span class="routeros">Deploy the entire tile38<span class="hljs-built_in"> config </span>with `kubectl apply -f ./tile38.yaml`

&lt;br /&gt;
Verify with `kubectl <span class="hljs-builtin-name">get</span> deployment --namespace api` <span class="hljs-keyword">to</span> confirm that the correct number of instances are created.
</span></code></pre></div><p>$ kubectl get deployment --namespace api
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tile38-write                1         1         1            1           21m
tile38-read                 3         3         3            3           21m</p>
<div class="markdown-code"><pre><code class="lang-">

<span class="livecodeserver">You can also <span class="hljs-built_in">send</span> <span class="hljs-keyword">a</span> curl request <span class="hljs-built_in">from</span> inside <span class="hljs-keyword">the</span> cluster <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> master <span class="hljs-keyword">and</span> <span class="hljs-built_in">read</span> instances <span class="hljs-built_in">to</span> <span class="hljs-built_in">get</span> their `server` information.
</span></code></pre></div><p>$ curl tile38-read:9851/server
{&quot;ok&quot;:true,&quot;stats&quot;:{&quot;aof_size&quot;:6807927,&quot;avg_item_size&quot;:126,&quot;cpus&quot;:8,&quot;heap_released&quot;:0,&quot;heap_size&quot;:37048872,&quot;http_transport&quot;:true,&quot;id&quot;:&quot;30b9d6ddfda2d30018503ebg49e79a21&quot;,&quot;in_memory_size&quot;:7012733,&quot;max_heap_size&quot;:0,&quot;mem_alloc&quot;:37048872,&quot;num_collections&quot;:1,&quot;num_hooks&quot;:0,&quot;num_objects&quot;:25,&quot;num_points&quot;:292169,&quot;num_strings&quot;:0,&quot;pid&quot;:38461,&quot;pointer_size&quot;:8,&quot;read_only&quot;:false,&quot;threads&quot;:8},&quot;elapsed&quot;:&quot;1.738714ms&quot;}</p>
<div class="markdown-code"><pre><code class="lang-">
&lt;br /&gt;

You can confirm <span class="hljs-keyword">that</span> <span class="hljs-keyword">the</span> autoscaling <span class="hljs-keyword">is</span> working <span class="hljs-keyword">by</span> submitting high traffic <span class="hljs-keyword">to</span> <span class="hljs-keyword">the</span> instances <span class="hljs-keyword">and</span> checking <span class="hljs-keyword">with</span> `kubectl`

</code></pre></div><p>$ kubectl get deployment --namespace api
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tile38-write                1         1         1            1           45m
tile38-read                 10        6         6            6           45m</p>
<div class="markdown-code"><pre><code class="lang-"></code></pre></div><div class="post-fin"><img src="/images/logo.svg"></div><img src="https://stats.lacy.ninja/scaling-tile38-on-kubernetes/stat.gif"></div></div><footer class="footer-component"><div class="footer-content"><p>Scaling Tile38 with replication on kubernetes</p><a href="https://blog.codinghorror.com/the-magpie-developer/" target="_blank">The Magpie developer</a><br><br><div>logo magpie icon by</div><a href="https://thenounproject.com/phlehmann/" target="_blank">Philipp Lehmann</a></div><p>© 2017 magpiedev</p></footer><!-- script(src='/js/index.js')--></body></html>