<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta name="description" content="Scaling Tile38 with replication on kubernetes"><meta property="og:title" content="Scaling Tile38 on Kubernetes | Magpie dev"><meta property="og:site_name" content="Magpie dev"><meta property="og:type" content="website"><meta property="og:description" content="Scaling Tile38 with replication on kubernetes"><meta property="og:url" content="https://magpiedev.com"><meta property="og:image" content="https://magpiedev.com/pages/scaling-tile38-on-kubernetes/1.png?0.24594012010691535?0.9592324587639645"><link rel="stylesheet" href="/index.css?0.9592324587639645"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400"><link rel="shortcut icon" href="/images/logo.ico"><title>Scaling Tile38 on Kubernetes | Magpie dev</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-101774767-2', 'auto');
ga('send', 'pageview');</script></head><body><nav class="navbar"><div class="content"><a class="logo" href="/"><img class="image" src="/images/logo.svg"></a><a class="title" href="/">Magpie dev</a><div class="items"><a href="/posts">articles</a></div><!-- .items.right--><!--   a(href='/about' class=page_active === 'about' ? 'active' : '')--><!--     img(src='/images/github.svg')--></div></nav><div class="view"><div class="featured-box-component content-page"><div class="post-title">Scaling Tile38 on Kubernetes</div><div class="post-info"><div class="post-date">Aug 6th, 2018</div><div class="post-views"><div>Views:</div><iframe class="stats" src="https://stats.lacy.ninja/scaling-tile38-on-kubernetes/stats?output=text" scrolling="no"></iframe></div></div><div class="post-image" style="background-image: url(&quot;/pages/scaling-tile38-on-kubernetes/1.png?0.24594012010691535&quot;);"></div><h1 id="scaling-tile38-on-kubernetes">Scaling Tile38 on Kubernetes</h1>
<p><br /></p>
<p><a href="https://github.com/tidwall/tile38">Tile38</a> is a geospatial database written in golang. It is designed to compute gis data with high throughput.</p>
<p>For our system we needed to autoscale Tile38 from 1 instance on kubernetes to 50 or more depending on load and requests.</p>
<p>Since Tile38 uses mostly CPU for calculations, it is easy to add a CPU based horizontal autoscaler on kubernetes to change the number of replicas available.</p>
<h2 id="leader-follower-replication">leader/follower replication</h2>
<p>Tile38 supports basic leader/follower replication. The follower instances run the tile38 command:</p>
<p><code>FOLLOW leaderhost 9851</code></p>
<p>Where <code>leaderhost</code> is the IP or dns hostname of the leader, it will then autosync with the leader&#39;s AOF.</p>
<p>For the setup, there will be one leader read/write kubernetes Deployment with a service dedicated to it, and one follower/readonly Deployment with a service for it.</p>
<p>This diagram shows a general overview:</p>
<p><img src="./2.png" alt="diagram one -- kubernets leader/follower replication"></p>
<p><br />
<code>tile38.yaml</code></p>
<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 master service</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-attr">  namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> <span class="hljs-string">NodePort</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">9851</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> <span class="hljs-string">tile38-write</span>

<span class="hljs-meta">---</span></code></pre></div><p>That service will enable your API deployment to connect to <code>http://tile38-write:9851</code> or via tile38&#39;s redis adapter, <code>tile38-write:9851</code>.</p>
<p>The tile38 leader now needs a deployment of a single read/write Pod. This ensures that you are never writing to two instances that do not sync.</p>
<p><br />
<code>tile38.yaml</code></p>
<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 master deployment</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-attr">  namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - image:</span> <span class="hljs-string">"tile38/tile38:alpine"</span>
<span class="hljs-attr">          name:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">9851</span>
<span class="hljs-attr">              name:</span> <span class="hljs-string">tile38-write</span>
<span class="hljs-meta">---</span></code></pre></div><p>Once complete you can run <code>kubectl apply -f tile38.yaml</code> to setup the deployment and service. Verify that you can connect to it inside your cluster.</p>
<h4 id="now-for-the-autoscaling-read-follow-instances-">Now for the autoscaling read (follow) instances.</h4>
<p>This service and deployment is what will take the brunt of all read requests, at times each pod may be using 100% of it&#39;s assigned CPU. For this setup I assigned one full CPU core to each Pod.</p>
<p><br />
<code>tile38.yaml</code></p>
<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 read service</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">  namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> <span class="hljs-string">NodePort</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">9851</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> <span class="hljs-string">tile38-read</span>

<span class="hljs-meta">---</span></code></pre></div><p>Your instances can now connect to <code>http://tile38-read:9851</code> and all the requests will be balanced to all the Pods in the following Deployment:</p>
<p><br />
<code>tile38.yaml</code></p>
<div class="markdown-code"><pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-comment"># Tile38 read</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">  namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span> <span class="hljs-comment"># Initial replica count</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - image:</span> <span class="hljs-string">"stevelacy/tile38:alpine"</span> <span class="hljs-comment"># Custom image that contains the check.py</span>
<span class="hljs-attr">          name:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">9851</span>
<span class="hljs-attr">              name:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">          resources:</span>
<span class="hljs-attr">            limits:</span>
<span class="hljs-attr">              cpu:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">            requests:</span>
<span class="hljs-attr">              cpu:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">          imagePullPolicy:</span> <span class="hljs-string">Always</span>
<span class="hljs-attr">          lifecycle:</span>
<span class="hljs-attr">            postStart:</span>
<span class="hljs-attr">              exec:</span>
<span class="hljs-attr">                command:</span> <span class="hljs-string">["python",</span> <span class="hljs-string">"/app/check.py"</span><span class="hljs-string">]</span> <span class="hljs-comment"># This is a custom script to ensure the replica "follows" the leader host</span>
<span class="hljs-attr">          readinessProbe:</span> <span class="hljs-comment"># This ensures that this Pod does not show it's state as "ready" until it follows and fully connects to the leader host</span>
<span class="hljs-attr">            exec:</span>
<span class="hljs-attr">              command:</span>
<span class="hljs-bullet">                -</span> <span class="hljs-string">python</span>
<span class="hljs-bullet">                -</span> <span class="hljs-string">/app/check.py</span>
<span class="hljs-attr">            initialDelaySeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">            periodSeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">            timeoutSeconds:</span> <span class="hljs-number">30</span>
<span class="hljs-attr">            failureThreshold:</span> <span class="hljs-number">5</span></code></pre></div><p>This deployment is using a custom image, <a href="https://hub.docker.com/r/stevelacy/tile38/">stevelacy/tile38:alpine</a></p>
<p>That image is using a python script for confirming that the leader is online: <a href="https://github.com/stevelacy/tile38-kubernetes-readiness">check.py</a></p>
<p><br /></p>
<blockquote>
<p>Note: it checks to see that the leader tile38 instance has 10 or more boundaries in it&#39;s system. If you wish to have fewer than 10 boundaries please modify as needed.</p>
</blockquote>
<h4 id="autoscaling">Autoscaling</h4>
<p>The core kubernetes autoscaler does a decent job of monitoring the cpu levels of the Pods and changing the requested replica counts as needed.</p>
<p>This autoscaling config will ensure that the replicas will start at 1 and scale up to a max of 10 when the average cpu crosses over 80%.</p>
<p>When the cpu load drops below 80% it will slowly start removing the excess replicas until it achieves constant 80% load or just one remaining replica.</p>
<p><br />
<code>tile38.yaml</code></p>
<div class="markdown-code"><pre><code class="lang-undefined"><span class="hljs-meta">---</span>

<span class="hljs-comment"># Autoscale pods based on cpu usage</span>

<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">autoscaling/v2beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">HorizontalPodAutoscaler</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">tile38-autoscaler</span>
<span class="hljs-attr">  namespace:</span> <span class="hljs-string">api</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  scaleTargetRef:</span>
<span class="hljs-attr">    apiVersion:</span> <span class="hljs-string">apps/v1beta1</span>
<span class="hljs-attr">    kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">    name:</span> <span class="hljs-string">tile38-read</span>
<span class="hljs-attr">  minReplicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  maxReplicas:</span> <span class="hljs-number">10</span>
<span class="hljs-attr">  metrics:</span>
<span class="hljs-attr">    - type:</span> <span class="hljs-string">Resource</span>
<span class="hljs-attr">      resource:</span>
<span class="hljs-attr">        name:</span> <span class="hljs-string">cpu</span>
<span class="hljs-attr">        targetAverageUtilization:</span> <span class="hljs-number">80</span></code></pre></div><p>Deploy the entire tile38 config with <code>kubectl apply -f ./tile38.yaml</code></p>
<p><br />
Verify with <code>kubectl get deployment --namespace api</code> to confirm that the correct number of instances are created.</p>
<div class="markdown-code"><pre><code class="lang-undefined">$ kubectl get deployment --namespace api
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tile38-write                <span class="hljs-number">1</span>         <span class="hljs-number">1</span>         <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">21</span>m
tile38-read                 <span class="hljs-number">3</span>         <span class="hljs-number">3</span>         <span class="hljs-number">3</span>            <span class="hljs-number">3</span>           <span class="hljs-number">21</span>m</code></pre></div><p>You can also send a curl request from inside the cluster to the master and read instances to get their <code>server</code> information.</p>
<div class="markdown-code"><pre><code class="lang-undefined">$ curl tile38-<span class="hljs-built_in">read</span>:<span class="hljs-number">9851</span>/server
{<span class="hljs-string">"ok"</span>:<span class="hljs-literal">true</span>,<span class="hljs-string">"stats"</span>:{<span class="hljs-string">"aof_size"</span>:<span class="hljs-number">6807927</span>,<span class="hljs-string">"avg_item_size"</span>:<span class="hljs-number">126</span>,<span class="hljs-string">"cpus"</span>:<span class="hljs-number">8</span>,<span class="hljs-string">"heap_released"</span>:<span class="hljs-number">0</span>,<span class="hljs-string">"heap_size"</span>:<span class="hljs-number">37048872</span>,<span class="hljs-string">"http_transport"</span>:<span class="hljs-literal">true</span>,<span class="hljs-string">"id"</span>:<span class="hljs-string">"30b9d6ddfda2d30018503ebg49e79a21"</span>,<span class="hljs-string">"in_memory_size"</span>:<span class="hljs-number">7012733</span>,<span class="hljs-string">"max_heap_size"</span>:<span class="hljs-number">0</span>,<span class="hljs-string">"mem_alloc"</span>:<span class="hljs-number">37048872</span>,<span class="hljs-string">"num_collections"</span>:<span class="hljs-number">1</span>,<span class="hljs-string">"num_hooks"</span>:<span class="hljs-number">0</span>,<span class="hljs-string">"num_objects"</span>:<span class="hljs-number">25</span>,<span class="hljs-string">"num_points"</span>:<span class="hljs-number">292169</span>,<span class="hljs-string">"num_strings"</span>:<span class="hljs-number">0</span>,<span class="hljs-string">"pid"</span>:<span class="hljs-number">38461</span>,<span class="hljs-string">"pointer_size"</span>:<span class="hljs-number">8</span>,<span class="hljs-string">"read_only"</span>:<span class="hljs-literal">false</span>,<span class="hljs-string">"threads"</span>:<span class="hljs-number">8</span>},<span class="hljs-string">"elapsed"</span>:<span class="hljs-string">"1.738714ms"</span>}</code></pre></div><p><br /></p>
<p>You can confirm that the autoscaling is working by submitting high traffic to the instances and checking with <code>kubectl</code></p>
<div class="markdown-code"><pre><code class="lang-undefined">$ kubectl get deployment --namespace api
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tile38-write                <span class="hljs-number">1</span>         <span class="hljs-number">1</span>         <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">45</span>m
tile38-read                 <span class="hljs-number">10</span>        <span class="hljs-number">6</span>         <span class="hljs-number">6</span>            <span class="hljs-number">6</span>           <span class="hljs-number">45</span>m</code></pre></div><div class="post-fin"><img src="/images/logo.svg"></div><img src="https://stats.lacy.ninja/scaling-tile38-on-kubernetes/stat.gif"></div></div><footer class="footer-component"><div class="footer-content"><p>Scaling Tile38 with replication on kubernetes</p><a href="https://blog.codinghorror.com/the-magpie-developer/" target="_blank">The Magpie developer</a><br><br><div>logo magpie icon by</div><a href="https://thenounproject.com/phlehmann/" target="_blank">Philipp Lehmann</a></div><p>© 2017 magpiedev</p></footer><!-- script(src='/js/index.js')--></body></html>